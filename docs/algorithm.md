新しいダイナミックモンテカルロ木自己改善の理解に不可欠なモンテカルロ木探索(MCTS)のメカニズムについて詳しく説明します。

モンテカルロ木探索

モンテカルロ木探索(MCTS)は、ゲームや複雑な意思決定プロセスで広く使用される意思決定アルゴリズムで、探索木を構築し、結果をシミュレートして行動の価値を推定します。4つの主要な段階(Browne et al., 2012)を含みます: 可能性を最大化するためのUCT戦略に基づく選択、新しいノードが追加される拡張、可能な結果を予見するシミュレーション、シミュレーション結果に基づいてノード値を更新するバックプロパゲーション。通常、MCTSアルゴリズムは4つの異なる段階で構成されます:

- 選択: ルートから始まり、アルゴリズムは特定の戦略(例: UCT)に基づいて有望な子ノードをナビゲートし、葉ノードに到達するまで続けます。
- 拡張: 葉ノードで、それがゲームの終端状態を表さない限り、将来の可能な動きを示す1つ以上の実行可能な新しい子ノードが追加されます。
- シミュレーションまたは評価: 新しく追加されたノードから、アルゴリズムはゲームの結論に達するまでランダムに動きを選択する「ロールアウト」と呼ばれるランダムシミュレーションを実行し、ノードの潜在的可能性を評価します。
- バックプロパゲーション: シミュレーション後、結果(勝利、敗北、または引き分け)はルートまで伝播され、将来の決定に情報を与えるために通過した各ノードの統計データ(例: 勝利、敗北)が更新されます。

これらの段階を繰り返し反復することで、MCTSは意思決定木を段階的に構築し、状態空間の広大さのために最良の戦略を直接計算することが不可能なシナリオで最適な意思決定のための戦略を洗練します。

木に適用された上限信頼度アルゴリズムは、MCTSの選択段階で重要であり、以下を最大化する行動を選択することで探索と活用のバランスを取ります:

*UCT_j = X_j + C * sqrt(2 * ln N_C / N_j)*

ここで、X_j は行動 j の平均報酬、N_C は親ノードの総訪問回数、n_j はシミュレーションのためにノード j が訪問された回数、C は活用と探索のバランスを取る定数です。

MCT自己改善アルゴリズムは、モンテカルロ木探索(MCTS)と大規模言語モデルを統合し、数学問題の解答の反復的改善プロセスを探索木構造に抽象化したものです。この木のノードは異なるバージョンの回答を表し、エッジは改善の試みを表します。このアルゴリズムの動作ワークフローは、MCTSアルゴリズムの一般的なパターンに従います。具体的には、自己反省駆動の自己改善を使用して回答を改善し、モデルの自己報酬能力を使用して異なる回答バージョンの報酬をサンプリングします。

MCTSrアルゴリズムの理解を促進するために、以下の記号と関数を定義します:

- *P* : 取り組んでいる問題インスタンス。
- *A*: *P* に対する潜在的な回答を表すノードのセット。
- *M* : 各ノードで利用可能なアクション、回答に対する可能な自己改善修正を表すセット。
- *R*: 修正の質と効果に基づいてノードの自己報酬をサンプリングする関数。
- *R_a*: 自己報酬関数 *R* を用いたノード a のすべての自己報酬サンプリング結果を格納するセット。
- *T* : 最大反復回数の到達や満足のいく回答品質の達成など、基準に基づいて探索プロセスの終了を決定する関数。
- *Q*(a): ノード a の価値を推定する価値関数、累積報酬 *R_a* と子ノードからのバックプロパゲーションから導出。
- *U*(a): ノード a の Q 値の上限信頼度、活用と探索のバランスを取るため。
- Father(a): 与えられたノード a の親ノードを返す関数。a がルートノードの場合、この関数は null または特定の識別子を返します。
- Children(a): 与えられたノード a のすべての子ノードのセットを返す関数、アクション m ∈ *M* を実行することで a から導出されるすべての可能な状態を表します。
- *N*(a): ノード a の総訪問回数、UCB 値の計算と探索と活用の状態の評価に使用されます。各訪問で報酬をサンプリングするため、この値は |*R_a*| に等しくなります。